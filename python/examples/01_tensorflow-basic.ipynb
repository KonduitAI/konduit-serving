{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow model serving with Konduit Serving\n",
    "\n",
    "This notebook illustrates a simple client-server interaction to perform inference on a TensorFlow model using the Python SDK for Konduit Serving.  \n",
    "\n",
    "This tutorial is split into two parts: \n",
    "\n",
    "1. Configuration \n",
    "2. Running the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konduit import ParallelInferenceConfig, ServingConfig, TensorFlowConfig, ModelConfigType\n",
    "from konduit import TensorDataTypesConfig, ModelPipelineStep, InferenceConfiguration\n",
    "from konduit.server import Server\n",
    "from konduit.client import Client\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Konduit Serving works by defining a series of \"pipeline steps\". These include operations such as \n",
    "1. Pre- or post-processing steps\n",
    "2. One or more machine learning models\n",
    "3. Transforming the output in a way that can be understood by humans\n",
    "\n",
    "If deploying your model does not require pre- nor post-processing, only one pipeline step - a machine learning model - is required. This configuration is defined using a single `ModelPipelineStep`. \n",
    "\n",
    "Before running this notebook, you should run the `build_jar.py` script and copy the JAR (`konduit.jar`) to this folder. Refer to the [Python SDK README](https://github.com/KonduitAI/konduit-serving/blob/master/python/README.md) for details. \n",
    "\n",
    "Start by downloading the model weights to the `data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve \n",
    "from zipfile import ZipFile\n",
    "urlretrieve(\"https://deeplearning4jblob.blob.core.windows.net/testresources/bert_mrpc_frozen_v1.zip\", \"../data/bert.zip\")\n",
    "with ZipFile('../data/bert.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring `ModelPipelineStep` \n",
    "\n",
    "Define the TensorFlow configuration as a `TensorFlowConfig` object. \n",
    "\n",
    "- `tensor_data_types_config`: The TensorFlowConfig object requires a dictionary `input_data_types`. Its keys should represent column names, and the values should represent data types as strings, e.g. `\"INT32\"`. See [here](https://github.com/KonduitAI/konduit-serving/blob/master/konduit-serving-api/src/main/java/ai/konduit/serving/model/TensorDataType.java) for a list of supported data types. \n",
    "- `model_config_type`: This argument requires a `ModelConfigType` object. Specify `model_type` as `TENSORFLOW`, and `model_loading_path` to point to the location of TensorFlow weights saved in the PB file format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_types = {'IteratorGetNext:0': 'INT32',\n",
    "                    'IteratorGetNext:1': 'INT32',\n",
    "                    'IteratorGetNext:4': 'INT32'}\n",
    "\n",
    "tensorflow_config = TensorFlowConfig(\n",
    "    tensor_data_types_config = TensorDataTypesConfig(input_data_types = input_data_types),\n",
    "    model_config_type = ModelConfigType(model_type = 'TENSORFLOW',\n",
    "                                        model_loading_path = '../data/bert_mrpc_frozen.pb')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `TensorFlowConfig` defined, we can define a `ModelPipelineStep`. The following parameters are specified: \n",
    "- `model_config`: pass the TensorFlowConfig object here \n",
    "- `parallel_inference_config`: specify the number of workers to run in parallel. Here, we specify `workers = 1`.\n",
    "- `input_names`:  names for the input data  \n",
    "- `output_names`: names for the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = list(input_data_types.keys())\n",
    "output_names = [\"loss/Softmax\"]\n",
    "\n",
    "tf_pipeline_step = ModelPipelineStep(model_config = tensorflow_config,\n",
    "                                     parallel_inference_config = ParallelInferenceConfig(workers = 1),\n",
    "                                     input_names = input_names,\n",
    "                                     output_names = output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the server\n",
    "\n",
    "Specify the following:\n",
    "- `http_port`: select a random port.\n",
    "- `input_data_type`, `output_data_type`: Specify input and output data types as strings. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "ℹ Accepted input and output data types are as follows: \n",
    "    <ul>\n",
    "        <li> Input: JSON, ARROW, IMAGE, ND4J (not yet implemented) and NUMPY. </li>\n",
    "        <li> Output: NUMPY, JSON, ND4J (not yet implemented) and ARROW.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = np.random.randint(1000, 65535)\n",
    "serving_config = ServingConfig(http_port = port,\n",
    "                               input_data_type = 'NUMPY',\n",
    "                               output_data_type = 'NUMPY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ServingConfig` has to be passed to `InferenceConfiguration`, in addition to the pipeline steps as a Python list. In this case, there is a single pipeline step: `tf_pipeline_step`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(config = InferenceConfiguration(serving_config = serving_config,\n",
    "                                                pipeline_steps = [tf_pipeline_step]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `Server()` looks for the Konduit Serving JAR `konduit.jar` in the directory the script is run in. To change this default, use the `jar_path` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration is stored as a dictionary. Note that the configuration can be converted to a dictionary using the `as_dict()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@type': 'InferenceConfiguration',\n",
       " 'pipelineSteps': [{'@type': 'ModelPipelineStep',\n",
       "   'inputNames': ['IteratorGetNext:0',\n",
       "    'IteratorGetNext:1',\n",
       "    'IteratorGetNext:4'],\n",
       "   'outputNames': ['loss/Softmax'],\n",
       "   'modelConfig': {'@type': 'TensorFlowConfig',\n",
       "    'tensorDataTypesConfig': {'@type': 'TensorDataTypesConfig',\n",
       "     'inputDataTypes': {'IteratorGetNext:0': 'INT32',\n",
       "      'IteratorGetNext:1': 'INT32',\n",
       "      'IteratorGetNext:4': 'INT32'}},\n",
       "    'modelConfigType': {'@type': 'ModelConfigType',\n",
       "     'modelType': 'TENSORFLOW',\n",
       "     'modelLoadingPath': '../data/bert_mrpc_frozen.pb'}},\n",
       "   'parallelInferenceConfig': {'@type': 'ParallelInferenceConfig',\n",
       "    'workers': 1}}],\n",
       " 'servingConfig': {'@type': 'ServingConfig',\n",
       "  'httpPort': 41551,\n",
       "  'inputDataType': 'NUMPY',\n",
       "  'outputDataType': 'NUMPY'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.config.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the client \n",
    "\n",
    "To configure the client, create a Client object with the following arguments: \n",
    "- `input_names`: names of the input data\n",
    "- `output_names`: names of the output data\n",
    "- `input_type`: data type passed to the server for inference\n",
    "- `endpoint_output_type`: data type returned by the server endpoint \n",
    "- `return_output_type`: data type to be returned to the client. Note that this argument can be used to convert the output returned from the server to the client into a different format, e.g. NUMPY to JSON.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    ⚠ Future versions of the Python SDK may remove the <code>input_names</code> and <code>output_names</code> arguments in <code>Client()</code>, since these are already specified in <code>ModelPipelineStep()</code>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(input_names = input_names,\n",
    "                output_names = output_names,\n",
    "                input_type = 'NUMPY',\n",
    "                endpoint_output_type = 'NUMPY',\n",
    "                return_output_type = \"NUMPY\",\n",
    "                url = 'http://localhost:' + str(port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some sample data from NumPy files. Note that these are NumPy arrays, each with shape (4, 128): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = {\n",
    "    'IteratorGetNext:0': np.load('../data/input-0.npy'),\n",
    "    'IteratorGetNext:1': np.load('../data/input-1.npy'),\n",
    "    'IteratorGetNext:4': np.load('../data/input-4.npy')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the server and wait 60 seconds for the server to start before the client requests the server for a prediction using the `data_input`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config.json to path C:\\Users\\Skymind AI Berhad\\Documents\\pk_konduit-serving\\python\\examples\\config.json\n",
      "Running with args\n",
      "java -cp konduit.jar ai.konduit.serving.configprovider.KonduitServingMain --configPath C:\\Users\\Skymind AI Berhad\\Documents\\pk_konduit-serving\\python\\examples\\config.json --verticleClassName ai.konduit.serving.verticles.inference.InferenceVerticle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skymind AI Berhad\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\konduit-0.1.1-py3.7.egg\\konduit\\client.py:150: FutureWarning: Possible nested set at position 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.996409   0.00359104]\n",
      " [0.97321105 0.02678899]\n",
      " [0.9955929  0.00440712]\n",
      " [0.9962774  0.00372254]]\n"
     ]
    }
   ],
   "source": [
    "server.start()\n",
    "time.sleep(60)\n",
    "\n",
    "predicted = client.predict(data_input)\n",
    "print(predicted)\n",
    "\n",
    "server.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
